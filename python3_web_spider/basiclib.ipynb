{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 基本库的使用\n",
    "\n",
    "## 3.1 urllib\n",
    "\n",
    "urllib 是 Python 内置的 HTTP 请求库，包含四个模块：\n",
    "* request: 它是最基本的 HTTP 请求模块，可以用来模拟发送请求。\n",
    "* error: 异常处理模块，如果出现请求错误，我们可以捕获这些异常。\n",
    "* parse: 工具模块，提供许多 URL 处理方法，比如拆分、解析、合并等。\n",
    "* robotparser: 主要是用来识别网站的 robots.txt 文件，判断哪些网页可以爬，哪些不可以爬。\n",
    "\n",
    "### 3.1.1 发送请求\n",
    "\n",
    "使用 urllib 的 request 模块，可以方便地实现请求的发送并得到响应。\n",
    "\n",
    "#### 1. urlopen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen('https://www.python.org')\n",
    "print(response.read().decode('utf-8'))  # read() 可以得到返回的网页内容\n",
    "print(type(response))  # <class 'http.client.HTTPResponse'>\n",
    "print(response.status)  # 200\n",
    "print(response.getheaders())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以给链接传递一些参数，urllib()函数的 API:\n",
    "```\n",
    "urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None)\n",
    "```\n",
    "* data 参数：可选，需要使用 bytes() 将参数转化为 bytes 类型。另外，需使用 POST 请求方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"word\": \"hello\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"10\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.7\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"114.240.125.16, 114.240.125.16\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf-8')\n",
    "response = urllib.request.urlopen('http://httpbin.org/post', data=data)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，传递的参数出现在 form 字段中，这表明是模拟了表单提交的方式，以 POST 方式传输数据。\n",
    "* timeout 参数：设置超时时间，单位为秒。如果超时无响应，会抛异常。默认使用全局默认时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 其它参数: context 参数，必须是 ssl.SSLContext 类型，用来指定 SSL 设置，cafile, capath 分别指定 CA 证书和它的路径，在请求 HTTPS 链接时有用，cadefault 参数已弃用。\n",
    "\n",
    "#### 2. Request\n",
    "\n",
    "如果请求中需要加入 Headers 等信息，可以利用更强大的 Request 类来构建。Request 的构造方法如下：\n",
    "```\n",
    "class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)\n",
    "```\n",
    "* url：请求 url，必传参数，其它都是可选参数；\n",
    "* data：如果要传，必须是 bytes 类型，可用 urllib.parse 模块里的 urlencode() 进行编码；\n",
    "* headers：请求头，可以在构造请求时通过 headers 参数直接构造，也可以通过调用请求实例的 add_header() 方法构造；添加请求头最常用的用法就是通过修改 User-Agent 来伪装浏览器；\n",
    "* origin_req_host：请求方的 host 名称或 IP 地址；\n",
    "* unverifiable：表示这个请求是否是无法通过验证的，默认是 False；\n",
    "* method：请求方法，比如 GET、POST、和 PUT 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Germey\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"11\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"114.240.125.16, 114.240.125.16\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request, parse\n",
    "\n",
    "url = 'http://httpbin.org/post'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\n",
    "    'Host': 'httpbin.org'\n",
    "}\n",
    "dict = {\n",
    "    'name': 'Germey'\n",
    "}\n",
    "data = bytes(parse.urlencode(dict), encoding='utf-8')\n",
    "req = request.Request(url=url, data=data, headers=headers, method='POST')\n",
    "response = request.urlopen(req)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 高级用法 \n",
    "\n",
    "介绍两种高级类，一是 Handler，可以理解为各种处理器，有处理登陆验证的，有处理 Cookies 的，有处理代理设置的。urllib.request 模块里的 BaseHandler 类是其它 Handler 的父类，它提供了最基本的用法，例如 default_open()、protocol_request() 等。一些子类如下：\n",
    "* HTTPDefaultErrorHandler: 用于处理 HTTP 响应错误；\n",
    "* HTTPRedirectHandler: 用于处理重定向；\n",
    "* HTTPCookieProcessor: 用于处理 Cookies；\n",
    "* ProxyHandler: 用于设置代理，默认代理为空；\n",
    "* HTTPPasswordMgr: 用于管理密码，它维护了用户名和密码的表；\n",
    "* HTTPBasicAuthHandler: 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。\n",
    "\n",
    "另一个就是 OpenerDirector，可以称为 Opener。Opener 可以使用 open() 方法，和 urlopen() 一致。可以利用 Handler 来构建 Opener。\n",
    "\n",
    "* 验证\n",
    "\n",
    "有些网站需要输入用户名和密码，可以使用 HTTPBasicAuthHandler 来完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener\n",
    "from urllib.error import URLError\n",
    "\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "url = 'http://localhost:5000/'\n",
    "\n",
    "p = HTTPPasswordMgrWithDefaultRealm()\n",
    "p.add_password(None, url, username, password)\n",
    "auth_handler = HTTPBasicAuthHandler(p)\n",
    "opener = build_opener(auth_handler)\n",
    "\n",
    "try:\n",
    "    result = opener.open(url)\n",
    "    html = result.read().decode('utf-8')\n",
    "    print(html)\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import URLError\n",
    "from urllib.request import ProxyHandler, build_opener\n",
    "\n",
    "proxy_handler = ProxyHandler({\n",
    "    'http': 'http://127.0.0.1:9743',\n",
    "    'https': 'http://127.0.0.1:9743'\n",
    "})\n",
    "opener = build_opener(proxy_handler)\n",
    "try:\n",
    "    response = opener.open('http://baidu.com')\n",
    "    print(response.read().decode('utf-8'))\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.cookiejar, urllib.request\n",
    "\n",
    "cookie = http.cookiejar.CookieJar()\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "response = opener.open('http://www.baidu.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 处理异常\n",
    "\n",
    "比较常用的两个异常类有：\n",
    "* URLError: 来自 urllib 的 error 模块，继承自 OSError 类，有一个 reason 属性；\n",
    "* HTTPError: 是 URLError 的子类，专门来处理 HTTP 请求错误，有三个属性：1）code，返回 HTTP 状态码；2）reason，同父类一样，返回错误的原因；3）headers，返回请求头。\n",
    "\n",
    "### 3.1.3 解析链接\n",
    "\n",
    "#### 1. urlparse()\n",
    "\n",
    "该方法可以实现 URL 的识别和分段："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'urllib.parse.ParseResult'> ParseResult(scheme='http', netloc='baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "result = urlparse('http://baidu.com/index.html;user?id=5#comment')\n",
    "print(type(result), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 url http://baidu.com/index.html;user?id=5#comment, urlparse() 将其分成了6部分，解析时有特定的分隔符。比如，:// 前面是 scheme，代表协议；第一个 / 符号前面是 netloc，即域名，后面是 path，即访问路径；分号;后面是 params，代表参数；问号?后面是查询条件 query，一般用作 GET 类型的 URL；井号 # 后面是锚点，用于直接定位页面内部的下来位置。所以，一个标准的链接合适为：scheme://netloc/path;params?query#fragment。\n",
    "\n",
    "#### 2. urlunparse()\n",
    "\n",
    "参数是一个可迭代对象，长度必须是 6，否则会抛出参数数量不足或过多的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/index.html;user?a=6#comment\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlunparse\n",
    "\n",
    "data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']\n",
    "print(urlunparse(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它方法，有 urlsplit()，同 urlparse()，该方法不单独解析 params 这一部分，只返回 5 个结果；urlunsplit() 同 urlunparse()，区别是长度必须是 5；urljoin()，可以实现链接的解析、拼合和生成。urlencode() 将字典数据序列化为 GET 请求参数，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.comname=germey&age=22\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlencode\n",
    "\n",
    "params = {\n",
    "    'name': 'germey',\n",
    "    'age': 22\n",
    "}\n",
    "base_url = 'http://www.baidu.com'\n",
    "url = base_url + urlencode(params)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与此相反的是反序列化函数 parse_qs()，可以将 GET 请求参数转化为字典，parse_qsl() 可以将参数转为化元组组成的列表。还有一个重要的函数，quote()，可以将内容转化为 URL 编码的格式。URL 中有中文时，有时可能会导致乱码，可以用该方法转化为 URL 编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import quote\n",
    "\n",
    "keyword = '壁纸'\n",
    "url = 'https://www.baidu.com/s?wd=' + quote(keyword)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与此相反，unquote() 可以进行 URL 解码。\n",
    "\n",
    "### 3.1.3 分析 Robots 协议\n",
    "\n",
    "#### 1. Robots 协议\n",
    "\n",
    "Robots 协议也称为爬虫协议、机器人协议，用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个 robots.txt 的文本文件，一般放在网站的根目录下。一个 robots.txt 的样例为：\n",
    "```\n",
    "User-agent:*\n",
    "Disallow: /\n",
    "Allow: /public/\n",
    "```\n",
    "\n",
    "#### 2. robotparser\n",
    "\n",
    "urllib.robotparse 模块提供了一个类 RobotFileParser，它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。该类声明为：\n",
    "```\n",
    "urllib.robotparse.RobotFileParser(url='')\n",
    "```\n",
    "常用方法为：\n",
    "* ser_url(): 设置 robots.txt 文件的链接，如果创建 RobotFileParser 对象时传入了链接，则不需要再设；\n",
    "* read(): 读取 robots.txt 文件并进行分析，该方法一定要调用，不然接下来的判断都为 False；\n",
    "* parse(): 用来解析 robots.txt 文件；\n",
    "* can_fetch(): 该方法传入两个参数，第一个是 User-agent，第二个是要抓取的 URL，返回 True 或 False；\n",
    "* mtime(): 返回上次抓取和分析 robots.txt 的时间；\n",
    "* modified(): 将当前时间设置为上次抓取和分析 robots.txt 的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url('http://www.jianshu.com/robots.txt')\n",
    "rp.read()\n",
    "print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 使用 requents\n",
    "\n",
    "urllib 其实不太好用，比如处理网页验证和 Cookies 时需要写 Opener 和 Handler 来处理，最好使用更强大的库 requests。\n",
    "\n",
    "### 3.2.1 基本用法\n",
    "\n",
    "urllib 库中的 urlopen() 方法实际上是以 GET 方式请求网页，而 requests 中相应的方法就是 get():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "200\n",
      "<class 'str'>\n",
      "<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://www.baidu.com')\n",
    "print(type(r))\n",
    "print(r.status_code)\n",
    "print(type(r.text))\n",
    "print(r.cookies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它请求依然可以用一行代码来完成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://httpbin.org/post')\n",
    "r = requests.put('http://httpbin.org/put')\n",
    "r = requests.delete('http://httpbin.org/delete')\n",
    "r = requests.head('http://httpbin.org/get')\n",
    "r = requests.options('http://httpbin.org/get')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. GET 请求\n",
    "\n",
    "详细了解下用 requests 构建 GET 请求。\n",
    "\n",
    "如果请求中附加额外信息的话，用 params 这个参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {\n",
      "    \"age\": \"22\", \n",
      "    \"name\": \"germey\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\"\n",
      "  }, \n",
      "  \"origin\": \"114.244.76.172, 114.244.76.172\", \n",
      "  \"url\": \"https://httpbin.org/get?name=germey&age=22\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = {\n",
    "    'name': 'germey',\n",
    "    'age': 22\n",
    "}\n",
    "r = requests.get('http://httpbin.org/get', params=data)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，请求的链接自动被构造成了：https://httpbin.org/get?name=germey&age=22 。\n",
    "\n",
    "另外，网页返回的结果是 JSON 格式的字符串，可以直接调用 json() 得到字典格式。但需要注意，如果返回的结果不是 JSON 格式，会抛出 json.decoder.JSONDecoderError。\n",
    "\n",
    "**抓取二进制数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://github.com/favicon.ico')\n",
    "with open('favicon.ico', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**添加 headers**\n",
    "\n",
    "通过 headers 参数来传递头信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36'\n",
    "}\n",
    "r = requests.get('https://www.zhihu.com/explore', headers=headers)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Post 请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"age\": \"22\", \n",
      "    \"data\": \"germey\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"18\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"114.244.76.172, 114.244.76.172\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = {'data': 'germey', 'age': '22'}\n",
    "r = requests.post('http://httpbin.org/post', data=data)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://www.jianshu.com')\n",
    "print(r.status_code)\n",
    "print(r.headers)\n",
    "print(r.cookies)\n",
    "print(r.url)\n",
    "print(r.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 高级用法\n",
    "\n",
    "#### 1. 文件上传"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "files = {'file': open('favicon.ico', 'rb')}\n",
    "r = requests.post('http://httpbin.org/post', files=files)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cookies\n",
    "\n",
    "前面 urllib 处理过 Cookies，写法比较复杂，而有了 requests，获取和设置 Cookies 只需一步即可完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://www.baidu.com')\n",
    "print(r.cookies)\n",
    "for key, value in r.cookies.items():\n",
    "    print('{}={}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以直接用 Cookie 来维持登陆状态。以知乎为例，登陆知乎后，将网页中 Headers 中的 Cookie 内容复制下来，设置到 Headers 里面，然后发送请求。示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Cookie': '***',\n",
    "    'Host': 'www.zhihu.com',\n",
    "    'User-Agent': '***'\n",
    "}\n",
    "r = requests.get('https://www.zhihu.com', headers=headers)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 会话维持\n",
    "\n",
    "可以利用 Session 对象来维持一个会话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"num\": \"123456789\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "s.get('http://httpbin.org/cookies/set/num/123456789')\n",
    "r = s.get('http://httpbin.org/cookies')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. SSL 证书验证\n",
    "\n",
    "当发送 HTTP 请求的时候，会检查 SSL 证书，可以使用 verify 参数控制是否检查此证书。如果不加 verify 参数的话，默认是 True，会自动验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('http://www.12306.cn', verify=False)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 代理设置\n",
    "\n",
    "对于大规模且频繁的请求，网站可能会弹出验证码，或跳转到登陆验证页面，为了防止这种情况，我们需要设置代理来解决这个问题，这就需要用到 proxies 参数。可以用这样的方式设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "proxies = {\n",
    "    'http': 'http://10.10.1.10:3128',\n",
    "    'https': 'http://10.10.1.10:1080'\n",
    "}\n",
    "requests.get('https://www.taobao.com', proxies=proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 超时设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://www.taobao.com', timeout=1)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 身份认证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "r = requests.get('http://localhost:5000', auth=HTTPBasicAuth('username', 'password'))\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 抓取猫眼电影排行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "\n",
    "def get_one_page(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def parse_one_page(html):\n",
    "    pattern = re.compile('<dd>.*?board-index.*?>(\\d+)</i>.*?data-src=\"(.*?)\".*?name\"><a'\n",
    "                         +'.*?>(.*?)</a>.*?star\">(.*?)</p>.*?releasetime\">(.*?)</p>'\n",
    "                         +'.*?integer\">(.*?)</i>.*?fraction\">(.*?)</i>.*?</dd>', re.S)\n",
    "    items = re.findall(pattern, html)\n",
    "    for item in items:\n",
    "        yield {\n",
    "            'index': item[0],\n",
    "            'image': item[1],\n",
    "            'title': item[2],\n",
    "            'actor': item[3].strip()[3:],\n",
    "            'time': item[4].strip()[5:],\n",
    "            'score': item[5] + item[6]\n",
    "        }\n",
    "\n",
    "    \n",
    "def write_to_file(content):\n",
    "    with open('movies.txt', 'r', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(content, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "def main(offset):\n",
    "    url = 'http://maoyan.com/board/4?offset=' + str(offset)\n",
    "    html = get_one_page(url)\n",
    "    for item in parse_one_page(html):\n",
    "        print(item)\n",
    "        write_to_file(item)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    for i in range(10):\n",
    "        main(offset=i * 10)\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
